{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6011ac02-4fef-4c4a-85df-cf4ab485bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from functions import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5716df5-14f8-475e-8cdd-29479b47cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2text(audios, seconds=30, noise=False):\n",
    "    r = sr.Recognizer()\n",
    "    archivos = os.listdir(audios)\n",
    "    datos = []\n",
    "\n",
    "    for archivo in archivos:\n",
    "        if archivo.endswith('.wav'):\n",
    "            audio_path = os.path.join(audios, archivo)\n",
    "            with sr.AudioFile(audio_path) as source:\n",
    "                duration = source.DURATION\n",
    "                splits = math.ceil(duration / seconds)\n",
    "            \n",
    "            transcripcion = \"\"  \n",
    "            \n",
    "            for i in tqdm(range(splits)):\n",
    "                with source as src:\n",
    "                    if noise:\n",
    "                        r.adjust_for_ambient_noise(src)\n",
    "                    audio = r.record(src, offset=i * seconds, duration=seconds)\n",
    "                \n",
    "                try:\n",
    "                    texto = r.recognize_google(audio, language=\"es-MX\")\n",
    "                    transcripcion += texto + \" \"\n",
    "                    speech_speed = get_speech_speed(audio_path)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            datos.append([archivo, transcripcion, speech_speed])\n",
    "            \n",
    "    df = pd.DataFrame(datos, columns=[\"Archivo\", \"Transcripcion\", \"Velocidad\"])\n",
    "    df = df.loc[df['Transcripcion'].notna() & (df['Transcripcion'] != '')]\n",
    "    df.to_csv(\"transcripciones.csv\", index=False, encoding='latin-1')\n",
    "\n",
    "\n",
    "#%time\n",
    "#carpeta_audio = \"C:/Users/DSTHREE/Documents/GITHUB/Speech2Text/GRABACIONES\"\n",
    "#convert2text(carpeta_audio, seconds=10) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99713b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech_speed(audio_path):\n",
    "    # Se carga el audio\n",
    "    audio, sr = librosa.load(audio_path)\n",
    "            \n",
    "    # Extraer segmentos del audio\n",
    "    speech_segments = librosa.effects.split(audio, top_db=20)\n",
    "            \n",
    "    # Calcular la duración de cada segmento\n",
    "    segment_durations = librosa.frames_to_time(\n",
    "        [segment[1] - segment[0] for segment in speech_segments],\n",
    "        sr=sr\n",
    "         )\n",
    "            \n",
    "    # Calculate the average speed of speech\n",
    "    average_speed = sum(segment_durations) / len(segment_durations)\n",
    "            \n",
    "    return average_speed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b05743e-5d5b-492e-8b40-fcda05e526f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]Exception ignored on calling ctypes callback function: <function ExecutionEngine._raw_object_cache_notify at 0x000001DC16234F70>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DSTHREE\\Documents\\GITHUB\\Speech2Text\\speech2txt\\lib\\site-packages\\llvmlite\\binding\\executionengine.py\", line 171, in _raw_object_cache_notify\n",
      "    def _raw_object_cache_notify(self, data):\n",
      "KeyboardInterrupt: \n",
      " 11%|█         | 4/36 [00:14<01:58,  3.72s/it]"
     ]
    }
   ],
   "source": [
    "%time\n",
    "carpeta_audio = \"Z:/CALIDAD/INTELIGENCIA_ARTIFICIAL/LLAMADAS_DIMEX\"\n",
    "convert2text(carpeta_audio, seconds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0394bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import speech_recognition as sr\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from pyAudioAnalysis import audioSegmentation\n",
    "import speech_recognition as sr\n",
    "\n",
    "def convertir_audio_a_texto(segmentos_audio):\n",
    "    r = sr.Recognizer()\n",
    "    texto_transcripcion = \"\"\n",
    "    for segmento in segmentos_audio:\n",
    "        try:\n",
    "            texto = r.recognize_google(segmento, language=\"es-MX\")\n",
    "            texto_transcripcion += texto + \" \"\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"No se pudo reconocer el audio.\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Error en la solicitud al servicio de reconocimiento de voz: {0}\".format(e))\n",
    "    return texto_transcripcion\n",
    "\n",
    "def get_speech_speed(audio_path):\n",
    "    # Se carga el audio\n",
    "    audio, sr = librosa.load(audio_path)\n",
    "            \n",
    "    # Extraer segmentos del audio\n",
    "    speech_segments = librosa.effects.split(audio, top_db=20)\n",
    "            \n",
    "    # Calcular la duración de cada segmento\n",
    "    segment_durations = librosa.frames_to_time(\n",
    "        [segment[1] - segment[0] for segment in speech_segments],\n",
    "        sr=sr\n",
    "         )\n",
    "            \n",
    "    # Calculate the average speed of speech\n",
    "    average_speed = sum(segment_durations) / len(segment_durations)\n",
    "            \n",
    "    return average_speed\n",
    "\n",
    "def convert2txt(audios, seconds=30, noise=False):\n",
    "    archivos = os.listdir(audios)\n",
    "    datos = []\n",
    "\n",
    "    for archivo in archivos:\n",
    "        if archivo.endswith('.wav'):\n",
    "            audio_path = os.path.join(audios, archivo)\n",
    "            with sr.AudioFile(audio_path) as source:\n",
    "                duration = librosa.get_duration(path=audio_path)\n",
    "                splits = math.ceil(duration / seconds)\n",
    "\n",
    "            # Obtener los segmentos de audio usando pyAudioAnalysis\n",
    "            audio, sampling_rate = librosa.load(audio_path, sr=None)\n",
    "            segmentos_habla = audioSegmentation.speaker_diarization(audio, sampling_rate)\n",
    "\n",
    "            segmentos_por_persona = {}\n",
    "            for segmento in segmentos_habla:\n",
    "                inicio, fin = segmento[0], segmento[1]\n",
    "                persona_id = segmento[2]\n",
    "                if persona_id not in segmentos_por_persona:\n",
    "                    segmentos_por_persona[persona_id] = []\n",
    "                segmentos_por_persona[persona_id].append(audio[inicio:fin])\n",
    "\n",
    "            # Transcribir los segmentos de cada persona\n",
    "            transcripciones_por_persona = {}\n",
    "            for persona_id, segmentos_persona in segmentos_por_persona.items():\n",
    "                transcripcion = convertir_audio_a_texto(segmentos_persona)\n",
    "                transcripciones_por_persona[persona_id] = transcripcion\n",
    "\n",
    "            speech_speed = get_speech_speed(audio_path)\n",
    "\n",
    "            datos.append([archivo, transcripciones_por_persona, speech_speed])\n",
    "\n",
    "    df = pd.DataFrame(datos, columns=[\"Archivo\", \"Transcripciones\", \"Velocidad\"])\n",
    "    df.to_csv(\"transcripciones.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "# Llamada a la función\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c4a1112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: file not found or other I/O error. (DECODING FAILED)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DSTHREE\\Documents\\GITHUB\\Speech2Text\\speech2txt\\lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 0.24.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\DSTHREE\\Documents\\GITHUB\\Speech2Text\\speech2txt\\lib\\site-packages\\pyAudioAnalysis\\ShortTermFeatures.py:17: RuntimeWarning: Mean of empty slice.\n",
      "  sig_array_norm -= sig_array_norm.mean()\n",
      "c:\\Users\\DSTHREE\\Documents\\GITHUB\\Speech2Text\\speech2txt\\lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m convert2txt(\u001b[39m\"\u001b[39;49m\u001b[39mZ:/CALIDAD/INTELIGENCIA_ARTIFICIAL/LLAMADAS_DIMEX\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[4], line 54\u001b[0m, in \u001b[0;36mconvert2txt\u001b[1;34m(audios, seconds, noise)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m# Obtener los segmentos de audio usando pyAudioAnalysis\u001b[39;00m\n\u001b[0;32m     53\u001b[0m audio, sampling_rate \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mload(audio_path, sr\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m---> 54\u001b[0m segmentos_habla \u001b[39m=\u001b[39m audioSegmentation\u001b[39m.\u001b[39;49mspeaker_diarization(audio, sampling_rate)\n\u001b[0;32m     56\u001b[0m segmentos_por_persona \u001b[39m=\u001b[39m {}\n\u001b[0;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m segmento \u001b[39min\u001b[39;00m segmentos_habla:\n",
      "File \u001b[1;32mc:\\Users\\DSTHREE\\Documents\\GITHUB\\Speech2Text\\speech2txt\\lib\\site-packages\\pyAudioAnalysis\\audioSegmentation.py:838\u001b[0m, in \u001b[0;36mspeaker_diarization\u001b[1;34m(filename, n_speakers, mid_window, mid_step, short_window, lda_dim, plot_res)\u001b[0m\n\u001b[0;32m    831\u001b[0m classifier_all, mean_all, std_all, class_names_all, _, _, _, _, _ \u001b[39m=\u001b[39m \\\n\u001b[0;32m    832\u001b[0m     at\u001b[39m.\u001b[39mload_model(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(base_dir, \u001b[39m\"\u001b[39m\u001b[39msvm_rbf_speaker_10\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    833\u001b[0m classifier_fm, mean_fm, std_fm, class_names_fm, _, _, _, _,  _ \u001b[39m=\u001b[39m \\\n\u001b[0;32m    834\u001b[0m     at\u001b[39m.\u001b[39mload_model(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(base_dir, \u001b[39m\"\u001b[39m\u001b[39msvm_rbf_speaker_male_female\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    837\u001b[0m mid_feats, st_feats, a \u001b[39m=\u001b[39m \\\n\u001b[1;32m--> 838\u001b[0m     mtf\u001b[39m.\u001b[39;49mmid_feature_extraction(signal, sampling_rate,\n\u001b[0;32m    839\u001b[0m                                mid_window \u001b[39m*\u001b[39;49m sampling_rate,\n\u001b[0;32m    840\u001b[0m                                mid_step \u001b[39m*\u001b[39;49m sampling_rate,\n\u001b[0;32m    841\u001b[0m                                \u001b[39mround\u001b[39;49m(sampling_rate \u001b[39m*\u001b[39;49m \u001b[39m0.05\u001b[39;49m),\n\u001b[0;32m    842\u001b[0m                                \u001b[39mround\u001b[39;49m(sampling_rate \u001b[39m*\u001b[39;49m \u001b[39m0.05\u001b[39;49m))\n\u001b[0;32m    844\u001b[0m mid_term_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((mid_feats\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(class_names_all) \u001b[39m+\u001b[39m\n\u001b[0;32m    845\u001b[0m                               \u001b[39mlen\u001b[39m(class_names_fm), mid_feats\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n\u001b[0;32m    846\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(mid_feats\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n",
      "File \u001b[1;32mc:\\Users\\DSTHREE\\Documents\\GITHUB\\Speech2Text\\speech2txt\\lib\\site-packages\\pyAudioAnalysis\\MidTermFeatures.py:94\u001b[0m, in \u001b[0;36mmid_feature_extraction\u001b[1;34m(signal, sampling_rate, mid_window, mid_step, short_window, short_step)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmid_feature_extraction\u001b[39m(signal, sampling_rate, mid_window, mid_step,\n\u001b[0;32m     88\u001b[0m                            short_window, short_step):\n\u001b[0;32m     89\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[39m    Mid-term feature extraction\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     short_features, short_feature_names \u001b[39m=\u001b[39m \\\n\u001b[1;32m---> 94\u001b[0m         ShortTermFeatures\u001b[39m.\u001b[39;49mfeature_extraction(signal, sampling_rate,\n\u001b[0;32m     95\u001b[0m                                              short_window, short_step)\n\u001b[0;32m     97\u001b[0m     n_stats \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     98\u001b[0m     n_feats \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(short_features)\n",
      "File \u001b[1;32mc:\\Users\\DSTHREE\\Documents\\GITHUB\\Speech2Text\\speech2txt\\lib\\site-packages\\pyAudioAnalysis\\ShortTermFeatures.py:570\u001b[0m, in \u001b[0;36mfeature_extraction\u001b[1;34m(signal, sampling_rate, window, step, deltas)\u001b[0m\n\u001b[0;32m    567\u001b[0m signal \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdouble(signal)\n\u001b[0;32m    568\u001b[0m signal \u001b[39m=\u001b[39m signal \u001b[39m/\u001b[39m (\u001b[39m2.0\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m15\u001b[39m)\n\u001b[1;32m--> 570\u001b[0m signal \u001b[39m=\u001b[39m dc_normalize(signal)\n\u001b[0;32m    572\u001b[0m number_of_samples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(signal)  \u001b[39m# total number of samples\u001b[39;00m\n\u001b[0;32m    573\u001b[0m current_position \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\DSTHREE\\Documents\\GITHUB\\Speech2Text\\speech2txt\\lib\\site-packages\\pyAudioAnalysis\\ShortTermFeatures.py:18\u001b[0m, in \u001b[0;36mdc_normalize\u001b[1;34m(sig_array)\u001b[0m\n\u001b[0;32m     16\u001b[0m sig_array_norm \u001b[39m=\u001b[39m sig_array\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     17\u001b[0m sig_array_norm \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m sig_array_norm\u001b[39m.\u001b[39mmean()\n\u001b[1;32m---> 18\u001b[0m sig_array_norm \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mabs\u001b[39;49m(sig_array_norm)\u001b[39m.\u001b[39;49mmax() \u001b[39m+\u001b[39m \u001b[39m1e-10\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m sig_array_norm\n",
      "File \u001b[1;32mc:\\Users\\DSTHREE\\Documents\\GITHUB\\Speech2Text\\speech2txt\\lib\\site-packages\\numpy\\core\\_methods.py:41\u001b[0m, in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_amax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     40\u001b[0m           initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_maximum(a, axis, \u001b[39mNone\u001b[39;49;00m, out, keepdims, initial, where)\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "convert2txt(\"Z:/CALIDAD/INTELIGENCIA_ARTIFICIAL/LLAMADAS_DIMEX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8184f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import speech_v1p1beta1 as speech  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "723ce7b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m client \u001b[39m=\u001b[39m speech\u001b[39m.\u001b[39;49mSpeechClient()\n\u001b[0;32m      2\u001b[0m speech_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mZ:/CALIDAD/INTELIGENCIA_ARTIFICIAL/LLAMADAS_DIMEX/ALLOPEZR_DIMEXENOJO.wav\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(speech_file, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m audio_file:\n",
      "File \u001b[1;32m~\\Documents\\GITHUB\\Speech2Text\\speech2txt\\Lib\\site-packages\\pyAudioAnalysis\\..\\google\\cloud\\speech_v1p1beta1\\services\\speech\\client.py:459\u001b[0m, in \u001b[0;36mSpeechClient.__init__\u001b[1;34m(self, credentials, transport, client_options, client_info)\u001b[0m\n\u001b[0;32m    454\u001b[0m     credentials \u001b[39m=\u001b[39m google\u001b[39m.\u001b[39mauth\u001b[39m.\u001b[39m_default\u001b[39m.\u001b[39mget_api_key_credentials(\n\u001b[0;32m    455\u001b[0m         api_key_value\n\u001b[0;32m    456\u001b[0m     )\n\u001b[0;32m    458\u001b[0m Transport \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mget_transport_class(transport)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transport \u001b[39m=\u001b[39m Transport(\n\u001b[0;32m    460\u001b[0m     credentials\u001b[39m=\u001b[39;49mcredentials,\n\u001b[0;32m    461\u001b[0m     credentials_file\u001b[39m=\u001b[39;49mclient_options\u001b[39m.\u001b[39;49mcredentials_file,\n\u001b[0;32m    462\u001b[0m     host\u001b[39m=\u001b[39;49mapi_endpoint,\n\u001b[0;32m    463\u001b[0m     scopes\u001b[39m=\u001b[39;49mclient_options\u001b[39m.\u001b[39;49mscopes,\n\u001b[0;32m    464\u001b[0m     client_cert_source_for_mtls\u001b[39m=\u001b[39;49mclient_cert_source_func,\n\u001b[0;32m    465\u001b[0m     quota_project_id\u001b[39m=\u001b[39;49mclient_options\u001b[39m.\u001b[39;49mquota_project_id,\n\u001b[0;32m    466\u001b[0m     client_info\u001b[39m=\u001b[39;49mclient_info,\n\u001b[0;32m    467\u001b[0m     always_use_jwt_access\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    468\u001b[0m     api_audience\u001b[39m=\u001b[39;49mclient_options\u001b[39m.\u001b[39;49mapi_audience,\n\u001b[0;32m    469\u001b[0m )\n",
      "File \u001b[1;32m~\\Documents\\GITHUB\\Speech2Text\\speech2txt\\Lib\\site-packages\\pyAudioAnalysis\\..\\google\\cloud\\speech_v1p1beta1\\services\\speech\\transports\\grpc.py:153\u001b[0m, in \u001b[0;36mSpeechGrpcTransport.__init__\u001b[1;34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ssl_channel_credentials \u001b[39m=\u001b[39m grpc\u001b[39m.\u001b[39mssl_channel_credentials(\n\u001b[0;32m    149\u001b[0m                 certificate_chain\u001b[39m=\u001b[39mcert, private_key\u001b[39m=\u001b[39mkey\n\u001b[0;32m    150\u001b[0m             )\n\u001b[0;32m    152\u001b[0m \u001b[39m# The base transport sets the host, credentials and scopes\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    154\u001b[0m     host\u001b[39m=\u001b[39;49mhost,\n\u001b[0;32m    155\u001b[0m     credentials\u001b[39m=\u001b[39;49mcredentials,\n\u001b[0;32m    156\u001b[0m     credentials_file\u001b[39m=\u001b[39;49mcredentials_file,\n\u001b[0;32m    157\u001b[0m     scopes\u001b[39m=\u001b[39;49mscopes,\n\u001b[0;32m    158\u001b[0m     quota_project_id\u001b[39m=\u001b[39;49mquota_project_id,\n\u001b[0;32m    159\u001b[0m     client_info\u001b[39m=\u001b[39;49mclient_info,\n\u001b[0;32m    160\u001b[0m     always_use_jwt_access\u001b[39m=\u001b[39;49malways_use_jwt_access,\n\u001b[0;32m    161\u001b[0m     api_audience\u001b[39m=\u001b[39;49mapi_audience,\n\u001b[0;32m    162\u001b[0m )\n\u001b[0;32m    164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grpc_channel:\n\u001b[0;32m    165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grpc_channel \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcreate_channel(\n\u001b[0;32m    166\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_host,\n\u001b[0;32m    167\u001b[0m         \u001b[39m# use the credentials which are saved\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m         ],\n\u001b[0;32m    179\u001b[0m     )\n",
      "File \u001b[1;32m~\\Documents\\GITHUB\\Speech2Text\\speech2txt\\Lib\\site-packages\\pyAudioAnalysis\\..\\google\\cloud\\speech_v1p1beta1\\services\\speech\\transports\\base.py:101\u001b[0m, in \u001b[0;36mSpeechTransport.__init__\u001b[1;34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m     credentials, _ \u001b[39m=\u001b[39m google\u001b[39m.\u001b[39mauth\u001b[39m.\u001b[39mload_credentials_from_file(\n\u001b[0;32m     98\u001b[0m         credentials_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mscopes_kwargs, quota_project_id\u001b[39m=\u001b[39mquota_project_id\n\u001b[0;32m     99\u001b[0m     )\n\u001b[0;32m    100\u001b[0m \u001b[39melif\u001b[39;00m credentials \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     credentials, _ \u001b[39m=\u001b[39m google\u001b[39m.\u001b[39mauth\u001b[39m.\u001b[39mdefault(\n\u001b[0;32m    102\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mscopes_kwargs, quota_project_id\u001b[39m=\u001b[39mquota_project_id\n\u001b[0;32m    103\u001b[0m     )\n\u001b[0;32m    104\u001b[0m     \u001b[39m# Don't apply audience if the credentials file passed from user.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(credentials, \u001b[39m\"\u001b[39m\u001b[39mwith_gdch_audience\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\Documents\\GITHUB\\Speech2Text\\speech2txt\\Lib\\site-packages\\pyAudioAnalysis\\..\\google\\auth\\_default.py:692\u001b[0m, in \u001b[0;36mdefault\u001b[1;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[0;32m    684\u001b[0m             _LOGGER\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    685\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mNo project ID could be determined. Consider running \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    686\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m`gcloud config set project` or setting the \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    687\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39menvironment variable\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    688\u001b[0m                 environment_vars\u001b[39m.\u001b[39mPROJECT,\n\u001b[0;32m    689\u001b[0m             )\n\u001b[0;32m    690\u001b[0m         \u001b[39mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[1;32m--> 692\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "client = speech.SpeechClient()\n",
    "speech_file = \"Z:/CALIDAD/INTELIGENCIA_ARTIFICIAL/LLAMADAS_DIMEX/ALLOPEZR_DIMEXENOJO.wav\"\n",
    "\n",
    "with open(speech_file, 'rb') as audio_file:\n",
    "    content = audio_file.read()\n",
    "audio = speech.RecognitionAudio(content = content)\n",
    "\n",
    "diarization_config = speech.SpeakerDiarizationConfig(\n",
    "    enable_speaker_diarization = True,\n",
    "    min_speaker_count=2,\n",
    "    max_speaker_count=10,\n",
    ")\n",
    "\n",
    "config = speech.RecognitionConfig(\n",
    "    encoding = speech.RecognitionConfig.AudioEncoding.LINEAR16, \n",
    "    sample_rate_hertz=8000,\n",
    "    language_code=\"es-MX\",\n",
    "    diarization_config=diarization_config,\n",
    ")\n",
    "\n",
    "print(\"Esperando a que la operación se complete\")\n",
    "response = client.recognize(config=config, audio=audio)\n",
    "result = response.results[-1]\n",
    "\n",
    "words_info = result.alternatives[0].words\n",
    "\n",
    "for word_info in words_info:\n",
    "    print(\n",
    "        f\"word: '{word_info.word}', speaker_tag: {word_info.speaker_tag}\"\n",
    "    )\n",
    "    \n",
    "return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
